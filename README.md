# terraforming the cloud - part 2

Temas abordados neste modulo:

* Cria√ß√£o de VPCs
* Cria√ß√£o de modulos de Terraform
* Cria√ß√£o de cluster GKE
* Cria√ß√£o de zonas de DNS


## 0. preparar o ambiente

### 0.1 cloud shell

**autenticar a consola com o GCP**
- Abrir o endere√ßo <https://console.cloud.google.com> e autenticar

```bash
gcloud config set project tf-gke-lab-01-np-000001
``` 

**facilitar a leitura dos ficheiros terraform em ambiente cloudshell**
Com o seguinte guia √© possivel ajudar um pouco na leitura sint√°tica dos ficheiros terraform usando a formata√ß√£o `ini`. 

Apesar de n√£o ser ideal, √© melhor do que n√£o ter nada e ajuda bastante!

- Com o editor aberto, carregar em `CTRL+,` para abrir as defini√ß√µes
- Procurar por `File Associations` e de seguida `Open settings.json`
- Garantir o seguinte bloco de `files.associations`:
```json
"files.associations": {
    "**/*.tf": "ini"
}
```
### 0.2 VSCode

```bash
gcloud init
gcloud auth application-default login 
``` 

### 0.3 preparar o projeto

**clonar o projecto git que vamos usar**
```bash
git clone https://github.com/nosportugal/terraforming-the-cloud-part2 && cd terraforming-the-cloud-part2
```

**obter e instalar a vers√£o do terraform e kubectl que vamos usar**
```bash
# terraform
sudo scripts/install-terraform.sh

# kubectl
sudo scripts/install-kubectl.sh
```

**inicializar o terraform**
```bash
# init & plan & apply
terraform init
terraform plan -out plan.tfplan
terraform apply plan.tfplan
```

## 1. criar a VPC

* No ficheiro [./vpc.tf](./vpc.tf) encontram-se as defini√ß√µes da VPC a usar

**Descomentar as seguintes resources**

```bash
# vpc
resource "google_compute_network" "default"
# subnet
resource "google_compute_subnetwork" "default"
```

**Plan & Apply**
```bash
terraform plan -out plan.tfplan
terraform apply plan.tfplan
```

*Validar the a VPC foi criada com a respetiva subnet...*
```bash
gcloud compute networks list
```

## 2. Modules & GKE

Neste capitulo iremos usar terraform modules para instanciar o GKE.

*[Como funcionam os modules?](https://www.terraform.io/docs/language/modules/syntax.html)*


### 2.1 GKE subnet

**Vamos precisar de uma subnet!**
* No ficheiro [./vpc.tf](./vpc.tf) encontram-se as defini√ß√µes da VPC a usar para o K8s
* Tamb√©m poderiamos configurar a subnet no modulo, mas dificulta a gest√£o transversal da VPC

```bash
# descomentar a seguinte resource
resource "google_compute_subnetwork" "gke"

# plan & apply
terraform plan -out plan.tfplan
terraform apply plan.tfplan
```

### 2.2 GKE module

* No ficheiro [./gke.tf](./gke.tf) encontra-se a defini√ß√£o do module
* Por cada module √© preciso fazer `terraform init`

```bash
# descomentar o module
module "gke"

# inicializar o modulo
terraform init

# plan & apply (vai demorar 5 minutos aprox)
terraform plan -out plan.tfplan
terraform apply plan.tfplan

# podem consultar os clusters ativos assim
gcloud container clusters list --project tf-gke-lab-01-np-000001 | grep $(terraform output -raw my_identifier)
```

* Ap√≥s o cluster estar UP, basta dirigirem-se a esta pagina: <https://console.cloud.google.com/kubernetes/list?project=tf-gke-lab-01-np-000001>
* Seleccionam o vosso cluster e voila!
* Mas e se n√≥s gerarmos a configura√ß√£o automaticamente...?


### 2.3 Kubeconfig module (module inside a module)

* Nesta sec√ß√£o iremos abordar a utiliza√ß√£o de modulos dentro de modules
* N√£o existe limita√ß√µes na profundidade das depend√™ncias, por√©m, √© preciso ter senso comum para evitar exageros pois o perfeccionismo √© um inimigo da funcionalidade.
* A desvantagem √© que apenas √© poss√≠vel passar raw-values, sendo sempre necess√°rio obter o data-object caso queiramos aceder a uma resource

**Objectivo: obter a configura√ß√£o `kubeconfig.yaml` de acesso ao cluster automaticamente**

* No ficheiro [./modules/gke/kubeconfig.tf](./modules/gke/kubeconfig.tf) encontra-se a defini√ß√£o do module
* **N√£o esquecer**: cada module novo √© preciso fazer `terraform init`

```bash
# descomentar o modulo kubeconfig e o respetivo output
module "kubeconfig"
output "gke_kubeconfig"

# init
terraform init

# plan & apply
terraform plan -out plan.tfplan
terraform apply plan.tfplan
```

**O que est√° a faltar? Continuamos sem instru√ß√µes de utiliza√ß√£o do `kubeconfig`...üò°**

* *O main module tamb√©m tem que emitir o valor...*

**No ficheiro [./gke.tf](./gke.tf) √© necess√°rio adicionar o novo `output` proveniente do module**

```bash
# descomentar o output
output "gke_kubeconfig"

# plan & apply
terraform plan -out plan.tfplan
terraform apply plan.tfplan

# dever√° existir um output identico a
export KUBECONFIG=kubeconfig.yaml

# se tiverem o `kubectl` instalado, basta fazerem isto para testarem o acesso ao cluster
kubectl get nodes
```

### 2.4. Vamos por workloads a correr?

* Nesta parte iremos usar o [kubectl](https://registry.terraform.io/providers/gavinbunney/kubectl/latest/docs) provider para instanciar todos os workloads de kubernetes

1. Habilitar o modulo [./k8s.tf](./k8s.tf) descomentando as linhas comentadas

```bash
# init
terraform init

# plan & apply
terraform plan -out plan.tfplan
terraform apply plan.tfplan

# ver os pods existentes
kubectl get pods -n hipster-demo

# fazer o port-forward
kubectl port-forward -n hipster-demo service/frontend 8080:80
```

* Ap√≥s este passo, basta testar a aplica√ß√£o no port-foward que foi estabelecido no seguinte url: <http://localhost:8080>
* Se estiverem a usar a Google CloudShell podem clicar em `Preview on Port 8080` no canto superior direito.

Portanto, conseguimos validar que os workloads estao a funcionar.
* O pr√≥ximo passo ser√° expor a partir dos ingresses e respectivos load-balancers do GKE
* Para isso precisamos de um DNS para HTTP/HTTPS
* Caso queiramos usar HTTPS vamos tamb√©m precisar de um certificado SSL

## 3. DNS

### 3.1 Criar a zona de DNS

* No ficheiro [./dns.tf](./dns.tf) encontra-se a defini√ß√£o do module
* **N√£o esquecer**: cada module novo √© preciso fazer `terraform init`

```bash
# descomentar o modulo e o output
module "dns"
output "fqdn"

# init
terraform init

# plan & apply
terraform plan -out plan.tfplan
terraform apply plan.tfplan
```

### 3.2 Criar um ponto de entrada (ingress) para o site

* No ficheiro [./modules/k8s/ingress.tf](./modules/k8s/ingress.tf) iremos descomentar a sec√ß√£o 3.2 que fazer com que seja aprovisionado um ingress para o nosso site.

```bash
# descomentar os seguintes
data "kubectl_path_documents" "hipster_ingress"
resource "kubectl_manifest" "hipster_ingress"

# plan & apply
terraform plan -out plan.tfplan
terraform apply plan.tfplan

# verificar a existencia de um ingress
kubectl get ingress -n hipster-demo
```
## 5. wrap-up & destroy

Destruir os conte√∫dos!

```bash

# destroy
terraform destroy
```

* **Nota:** se o destroy der erro √© porque o terraform n√£o consegue apagar um recurso devido a depend√™ncias externas. Isto pode acontecer devido aos recursos que foram criados pela ferramenta `kubectl`.
  * Se for este o caso, ent√£o ser√° necess√°rio remover os NEGs √† mao para o destroy funcionar.

## Comandos √∫teis

```bash
# listar
gcloud compute network-endpoint-groups list

# apagar
gcloud compute network-endpoint-groups delete <id>

# delete multiple negs at once
gcloud compute network-endpoint-groups delete $(gcloud compute network-endpoint-groups list --format="value(name)" --project tf-gke-lab-01-np-000001)
```