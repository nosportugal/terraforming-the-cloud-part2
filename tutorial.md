# terraforming the cloud - part 2

![Terraforming the cloud architecture][tfc-arch]

## Temas abordados neste modulo

* Cria√ß√£o de [modulos de Terraform](https://www.terraform.io/docs/language/modules/syntax.html)
* Cria√ß√£o de [cluster GKE](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster)
* Cria√ß√£o de [zonas de DNS](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/dns_managed_zone)
* Utiliza√ß√£o de diferentes providers ([kubectl provider](https://registry.terraform.io/providers/gavinbunney/kubectl/latest/docs))
* [Templates de ficheiros](https://registry.terraform.io/providers/hashicorp/template/latest/docs/data-sources/file)

**Tempo estimado**: Cerca de 2 horas

**Pr√© requisitos**: Antes de come√ßares, √© necess√°rio verificares algumas coisas:

Junta-te √† **Cloud Guild** no Teams e segue os tutorias da Wiki do GCP.

Depois pede para te adicionarem ao projeto no GCP.

De seguida vais ter de configurar o GCP. Se j√° tens o `gcloud` instalado corre este comando:

**NOTA: Se est√°s a executar tutorial na cloudshell (consola do GCP), n√£o precisas de correr este comando.**

```bash
gcloud auth application-default login
```

Podes econtrar mais info sobre a auth [aqui](https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/getting_started).

Certifica-te que tens a `google-cloud-shell` devidamente autorizada correndo este comando:

```bash
gcloud config set project tf-gke-lab-01-np-000001
```

De seguida, clica no bot√£o **Start** para come√ßares.

## 0. terraform `init`

Neste projeto temos que preparar um prefixo que identifique unicamente os recursos que v√£o ser criados, por forma a evitar colis√µes.

* No ficheiro `./terraform.tfvars` √© necess√°rio definir um prefixo, no seguinte formato: `user_prefix = "<valor>"`

Inicializar:

```bash
terraform init
```

Planear:

```bash
terraform plan -out plan.tfplan
```

Aplicar:

```bash
terraform apply plan.tfplan
```

## 1. criar a VPC

* No ficheiro `./vpc.tf` encontram-se as defini√ß√µes da VPC a usar

üëâ Descomentar as seguintes resources:

* `resource "google_compute_network" "default"`

*Tip: `CTRL+K+U` √© o atalho para descomentar em bloco*

Executar o `plan` & `apply`:

```bash
terraform plan -out plan.tfplan
```

```bash
terraform apply plan.tfplan
```

Validar the a VPC foi criada com a respetiva subnet:

```bash
gcloud compute networks list | grep $(terraform output -raw my_identifier)
```

## 2. Modules & GKE

Neste capitulo iremos abordar a utiliza√ß√£o de [terraform modules](https://www.terraform.io/docs/language/modules/syntax.html) para instanciar o GKE.

### 2.1 GKE subnet

Para aprovisionar um GKE √© necess√°rio uma subnet. Esta subnet ser√° usada para endere√ßar as as principais componentes do GKE: `pods`, `services` e `nodes`.

* No ficheiro `./vpc.tf` encontram-se as defini√ß√µes da VPC a usar para o GKE
* Tamb√©m poderiamos configurar a subnet no modulo, mas dificulta a gest√£o transversal da VPC

üëâ No ficheiro `./vpc.tf`, descomentar as seguintes resources:

* `resource "google_compute_subnetwork" "gke"`
* `resource "google_compute_router" "default"`
* `resource "google_compute_router_nat" "default"`

**Why**: Tanto o `router` como o `nat` s√£o recursos necess√°rios para permitir que o cluster GKE possa aceder √† internet para fazer download das imagens dos containers que vamos usar._

Executar o `plan` & `apply`:

```bash
terraform plan -out plan.tfplan
```

```bash
terraform apply plan.tfplan
```

Podemos verificar que a subnet foi corretamente criada:

```bash
gcloud compute networks subnets list --uri | grep "$(terraform output -raw my_identifier)"
```

### 2.2 GKE module

Agora que temos a subnet preparada, iremos entao proceder √† primeira aplica√ß√£o de um [terraform module](https://www.terraform.io/docs/language/modules/syntax.html) para aprovisionar um cluster GKE.

> *[from docs:](https://www.terraform.io/docs/language/modules/syntax.html) A module is a container for multiple resources that are used together.*
>
> *Every Terraform configuration has at least one module, known as its root module, which consists of the resources defined in the .tf files in the main working directory.*
>
> *A module can call other modules, which lets you include the child module's resources into the configuration in a concise way. Modules can also be called multiple times, either within the same configuration or in separate configurations, allowing resource configurations to be packaged and re-used.*

* No ficheiro `./gke.tf` encontra-se a invoca√ß√£o do module
* Por cada module √© preciso fazer `terraform init`

üëâ No ficheiro `./gke.tf`, descomentar as seguintes resources:

* `module "gke"`
* `output "gke_name"`
* `output "gke_location"`

Primeiro temos que executar `terraform init` para inicializar o modulo:

```bash
terraform init
```

Executar o `plan` & `apply`:

```bash
terraform plan -out plan.tfplan
```

```bash
terraform apply plan.tfplan
```

<sub>‚è∞ Notem que a cria√ß√£o de um cluster GKE pode levar at√© **10 minutos**...</sub>

Podemos verificar que o nosso cluster foi corretamente criado atrav√©s do comando:

```bash
gcloud container clusters list --project $(terraform output -raw project_id) | grep $(terraform output -raw my_identifier)
```

*Tamb√©m √© possivel verificar o estado do cluster pela GUI [aqui](https://console.cloud.google.com/kubernetes/list?project=tf-gke-lab-01-np-000001).*

### 2.3 Aceder ao cluster

O acesso a um GKE, tal como qualquer outro cluster de Kubernetes, √© feito a partir da cli `kubectl`. Para podermos executar comandos `kubectl` precisamos primeiro de garantir que temos uma configura√ß√£o v√°lida para aceder ao nosso cluster.

Usar o comando `gcloud` para construir um `KUBECONFIG` v√°lido para aceder ao cluster:

```bash
export KUBECONFIG=$(pwd)/kubeconfig.yaml && gcloud container clusters get-credentials $(terraform output -raw gke_name) --zone $(terraform output -raw gke_location) --project $(terraform output -raw project_id)
```

Verificar o acesso ao cluster:

```bash
kubectl get nodes
```

### 2.4. Vamos por workloads a correr

Nesta sec√ß√£o abordar a utiliza√ß√£o de um provider ([kubectl provider](https://registry.terraform.io/providers/gavinbunney/kubectl/latest/docs)) para instanciar (via terraform) todos os workloads que v√£o correr no nosso cluster.

Trata-se de um provider da comunidade que tal como o nome indica, facilita a utiliza√ß√£o de terraform para orquestrar ficheiros `yaml`.

> *[from docs:](https://registry.terraform.io/providers/gavinbunney/kubectl/latest/docs) This provider is the best way of managing Kubernetes resources in Terraform, by allowing you to use the thing Kubernetes loves best - yaml!*

üëâ Para habilitar o modulo, temos que ir ao ficheiro `./k8s.hipster.tf` e descomentar o seguinte modulo:

* `module "hipster"`
* ‚ùó‚ùó **n√£o** descomentar a linha `fqdn`; ser√° habilitado mais a frente ‚ùó‚ùó

Os microservi√ßos utilizados nesta demo, encontram-se [neste registry](https://console.cloud.google.com/gcr/images/google-samples/global/microservices-demo). Antes de inicializarem o m√≥dulo, verifiquem que as vers√µes destes microservi√ßos (passar pelos ficheiros `./hipster-demo/k8s/*.yaml` ) ainda existem.

Executar `terraform init` para inicializar o modulo:

```bash
terraform init
```

Executar o `plan` & `apply`:

```bash
terraform plan -out plan.tfplan
```

```bash
terraform apply plan.tfplan
```

<sub>‚è∞ Ap√≥s o `apply`, pode demorar uns minutos a acabar o comando pois o terraform espera que os pods estejam em `Running` state.</sub>

Podemos verificar que os pods foram corretamente instanciados:

```bash
kubectl get pods -n hipster-demo
```

Tamb√©m podemos constatar que o cluster-autoscaler teve que aprovisionar mais *nodes* para acautelar o demand üòÉüöÄ

```bash
kubectl get nodes
```

### 2.5. Testar a nossa aplica√ß√£o

Para testar e validar a nossa aplica√ß√£o antes de a colocar em "produ√ß√£o", podemos tirar partido da capacidade de fazer `port-forward`.

Para iniciar um `port-forward` no porto `8080`:

```bash
kubectl port-forward -n hipster-demo service/frontend 8080:80
```

* Ap√≥s este passo, basta testar a aplica√ß√£o no port-foward que foi estabelecido no seguinte url: <http://localhost:8080>
* Se estiverem a usar a Google CloudShell podem clicar em <walkthrough-web-preview-icon></walkthrough-web-preview-icon> `Preview on Port 8080` no canto superior direito

## 3. DNS & HTTPS

Conseguimos validar que os workloads estao a funcionar.

* O pr√≥ximo passo ser√° expor a partir dos ingresses e respectivos load-balancers do GKE
* Para isso precisamos de um DNS para HTTP/HTTPS
* Caso queiramos usar HTTPS vamos tamb√©m precisar de um certificado SSL

### 3.1 Criar a zona de DNS

No ficheiro `./dns.tf` encontra-se a defini√ß√£o do modulo.

üëâ Para habilitar o modulo `./dns.tf` precisamos de descomentar as seguintes resources:

* `module "dns"`
* `output "fqdn"`

Executar `terraform init` para inicializar o modulo:

```bash
terraform init
```

Executar o `plan` & `apply`:

```bash
terraform plan -out plan.tfplan
```

```bash
terraform apply plan.tfplan
```

Podemos verificar que a nossa zona de DNS foi corretamente criada atrav√©s do seguinte comando:

```bash
gcloud dns managed-zones list --project $(terraform output -raw project_id) | grep $(terraform output -raw my_identifier)
```

### 3.2 Habilitar o `external-dns`

O `external-dns` √© a *cola* entre o Kubernetes e o DNS.

> *[from docs:](https://github.com/kubernetes-sigs/external-dns) ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers.*

No ficheiro  √© necess√°rio passar o fqdn devolvido pelo modulo de dns.

üëâ Descomentar o modulo `external_dns` ficheiro `./k8s.external-dns.tf`:

* `module "external_dns"`
* `output "fqdn"`

Na diret√≥ria `./modules/external-dns` encontra-se a implementa√ß√£o do modulo `external-dns` que permite atualizar os registos DNS automaticamente.

### 3.3 Criar um ponto de entrada (ingress) para o site

> *[from docs:](https://kubernetes.io/docs/concepts/services-networking/ingress/) Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.*

A cria√ß√£o do `ingress` ser√° o culminar das √∫ltimas opera√ß√µes que efectuamos (DNS + HTTPS).

* S√≥ ser√° possivel aceder ao nosso site via internet se o expormos a partir de um ingress;
* A cria√ß√£o do ingress ir√° despoletar a cria√ß√£o de um balanceador com um IP p√∫blico bem como a gera√ß√£o de um certificado gerido pela Google;
* Ap√≥s a atribui√ß√£o do IP, o `external-dns` ir√° atualizar o DNS com o respetivo IP;
* Uma vez criado o registo no DNS, o GCE ir√° aprovisionar o certificado automaticamente;
* ‚è∞ Todo o processo pode levar at√© cerca de **10 minutos** a acontecer;

üëâ No ficheiro `./k8s.hipster.tf` iremos descomentar a sec√ß√£o **3.3** onde iremos modificar o comportamento do modulo da seguinte forma:

1. Atribuir o `fqdn` dado pelo modulo de `dns`  √° vari√°vel `fqdn`; o `fqdn` representa o dom√≠nio onde vai ser criado o host declarado pelo `ingress`.

   * `fqdn = module.dns.fqdn`

2. Ativar a cria√ß√£o dos manifestos de ingress atrav√©s da vari√°vel boleana `ingress_enabled`.

   * `ingress_enabled = true`

Executar o `plan` & `apply`:

```bash
terraform plan -out plan.tfplan
```

```bash
terraform apply plan.tfplan
```

Podemos verificar a cria√ß√£o do `ingress` e a respetiva atribui√ß√£o de IP a partir dos seguintes comandos:

```bash
kubectl get ingress -n hipster-demo
```

```bash
kubectl describe ingress -n hipster-demo hipster-ingress
```

Tamb√©m podemos verificar a atua√ß√£o do `external-dns` assim que o ingress ganhou um IP:

```bash
kubectl logs -f -n external-dns -l app=external-dns
```

Ou ent√£o podemos verificar os registos no Cloud DNS:

```bash
gcloud dns record-sets list --zone $(terraform output -raw my_identifier)-dns --project $(terraform output -raw project_id)
```

> üöÄ Infelizmente, devido ao tempo que a Google demora a gerar os certificados, o site s√≥ estar√° dispon√≠vel quando o certificado for gerado e a *chain* estiver devidamente validada. Este processo leva cerca de **10 minutos** ‚è∞üò°

Podemos verificar o estado do mesmo usando o seguinte comando:

```bash
kubectl describe managedcertificates -n hipster-demo hipster
```

## 4. wrap-up & destroy

Por fim, podemos destruir tudo de uma s√≥ vez.

‚è∞ Notem que devido √† quantidade de recursos envolvidos, a opera√ß√£o de destroy pode demorar entre **10 a 20 minutos**.

```bash
terraform destroy
```

üîöüèÅ Cheg√°mos ao fim üèÅüîö

<walkthrough-conclusion-trophy></walkthrough-conclusion-trophy>

<!-- markdownlint-disable-file MD013 -->
<!-- markdownlint-disable-file MD033 -->

 [//]: # (*****************************)
 [//]: # (INSERT IMAGE REFERENCES BELOW)
 [//]: # (*****************************)

[tfc-arch]: https://github.com/nosportugal/terraforming-the-cloud-part2-2022-autopilot/raw/main/images/terraforming-the-cloud.png "Terraforming the cloud architecture"
